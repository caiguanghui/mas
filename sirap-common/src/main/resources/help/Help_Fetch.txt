http://URLxxx.pdf  it is a typical url indicating a normal file, fetch the file and save to misc folder
http://URLxxx.pdf $+web, to view given web page
http://URLxxx.pdf $-web, to download given web page
f E:\Links.txt     the text file could contain lines of URLs indicating specified resources, parse them
f E:\Links.txt     and fetch resource one by one
=http://www.a.cn  get headers for give connection by url
==http://www.a.cn   display html source code in pretty format, same as cURL, option: $+post for POST 
===http://www.a.cn  display html source code in raw format, kind of ugly, option: $+post for POST request
